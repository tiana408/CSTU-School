{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f086e47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/18 03:22:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/02/18 03:22:52 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/02/18 03:22:52 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark GBTRegressor example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d0c021e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---------+-----+\n",
      "|   TV|Radio|Newspaper|Sales|\n",
      "+-----+-----+---------+-----+\n",
      "|230.1| 37.8|     69.2| 22.1|\n",
      "| 44.5| 39.3|     45.1| 10.4|\n",
      "| 17.2| 45.9|     69.3|  9.3|\n",
      "|151.5| 41.3|     58.5| 18.5|\n",
      "|180.8| 10.8|     58.4| 12.9|\n",
      "+-----+-----+---------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- TV: double (nullable = true)\n",
      " |-- Radio: double (nullable = true)\n",
      " |-- Newspaper: double (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('com.databricks.spark.csv').\\\n",
    "                               options(header='true', \\\n",
    "                               inferschema='true').\\\n",
    "                               load(\"data/Advertising.csv\",header=True);\n",
    "\n",
    "df.show(5,True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e63d7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/18 03:23:02 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+------------------+------------------+\n",
      "|summary|               TV|             Radio|         Newspaper|             Sales|\n",
      "+-------+-----------------+------------------+------------------+------------------+\n",
      "|  count|              200|               200|               200|               200|\n",
      "|   mean|         147.0425|23.264000000000024|30.553999999999995|14.022500000000003|\n",
      "| stddev|85.85423631490805|14.846809176168728| 21.77862083852283| 5.217456565710477|\n",
      "|    min|              0.7|               0.0|               0.3|               1.6|\n",
      "|    max|            296.4|              49.6|             114.0|              27.0|\n",
      "+-------+-----------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b608b39",
   "metadata": {},
   "source": [
    "Convert the data to dense vector (features and label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef09260b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|         features|label|\n",
      "+-----------------+-----+\n",
      "|[230.1,37.8,69.2]| 22.1|\n",
      "| [44.5,39.3,45.1]| 10.4|\n",
      "| [17.2,45.9,69.3]|  9.3|\n",
      "|[151.5,41.3,58.5]| 18.5|\n",
      "|[180.8,10.8,58.4]| 12.9|\n",
      "+-----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "transformed=df.rdd.map(lambda r: [Vectors.dense(r[:-1]),r[-1]]).toDF(['features','label'])\n",
    "transformed.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9627ba3c",
   "metadata": {},
   "source": [
    "Deal with the Categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f19f3eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+-----------------+\n",
      "|         features|label|  indexedFeatures|\n",
      "+-----------------+-----+-----------------+\n",
      "|[230.1,37.8,69.2]| 22.1|[230.1,37.8,69.2]|\n",
      "| [44.5,39.3,45.1]| 10.4| [44.5,39.3,45.1]|\n",
      "| [17.2,45.9,69.3]|  9.3| [17.2,45.9,69.3]|\n",
      "|[151.5,41.3,58.5]| 18.5|[151.5,41.3,58.5]|\n",
      "|[180.8,10.8,58.4]| 12.9|[180.8,10.8,58.4]|\n",
      "+-----------------+-----+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "featureIndexer = VectorIndexer(inputCol=\"features\", \\\n",
    "                               outputCol=\"indexedFeatures\",\\\n",
    "                               maxCategories=4).fit(transformed)\n",
    "\n",
    "data = featureIndexer.transform(transformed)\n",
    "data.show(5,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8b5836",
   "metadata": {},
   "source": [
    "Split the data into training and test sets (40% held out for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "682e0fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+---------------+\n",
      "|       features|label|indexedFeatures|\n",
      "+---------------+-----+---------------+\n",
      "| [0.7,39.6,8.7]|  1.6| [0.7,39.6,8.7]|\n",
      "| [5.4,29.9,9.4]|  5.3| [5.4,29.9,9.4]|\n",
      "|[7.3,28.1,41.4]|  5.5|[7.3,28.1,41.4]|\n",
      "|[7.8,38.9,50.6]|  6.6|[7.8,38.9,50.6]|\n",
      "| [8.4,27.2,2.1]|  5.7| [8.4,27.2,2.1]|\n",
      "+---------------+-----+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------------+-----+----------------+\n",
      "|        features|label| indexedFeatures|\n",
      "+----------------+-----+----------------+\n",
      "|  [4.1,11.6,5.7]|  3.2|  [4.1,11.6,5.7]|\n",
      "| [13.1,0.4,25.6]|  5.3| [13.1,0.4,25.6]|\n",
      "|[13.2,15.9,49.6]|  5.6|[13.2,15.9,49.6]|\n",
      "|[17.2,45.9,69.3]|  9.3|[17.2,45.9,69.3]|\n",
      "|[17.9,37.6,21.6]|  8.0|[17.9,37.6,21.6]|\n",
      "+----------------+-----+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and test sets (40% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.6, 0.4])\n",
    "\n",
    "trainingData.show(5)\n",
    "testData.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb76071",
   "metadata": {},
   "source": [
    "Fit RandomForest Regression Model with GBTRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "867271aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LinearRegression class\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "# Define LinearRegression algorithm\n",
    "gbt = GBTRegressor() #numTrees=2, maxDepth=2, seed=42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defeee59",
   "metadata": {},
   "source": [
    "If you decide to use the indexedFeatures features, you need to add the parameter featuresCol=\"indexedFeatures\".\n",
    "\n",
    "Pipeline Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ded60d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Chain indexer and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, gbt])\n",
    "model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12829ddb",
   "metadata": {},
   "source": [
    "Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46b2cf24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/18 03:23:59 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+-----------------+\n",
      "|        features|label|       prediction|\n",
      "+----------------+-----+-----------------+\n",
      "|  [4.1,11.6,5.7]|  3.2|5.132674876007165|\n",
      "| [13.1,0.4,25.6]|  5.3|5.145373662243048|\n",
      "|[13.2,15.9,49.6]|  5.6|6.992983868472848|\n",
      "|[17.2,45.9,69.3]|  9.3| 9.43868845548319|\n",
      "|[17.9,37.6,21.6]|  8.0|7.749033576102263|\n",
      "+----------------+-----+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"features\",\"label\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8412fb",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93e752db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 218:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 1.18684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a893d1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Squared (R2) on test data = 0.939112\n"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
    "                 labelCol=\"label\",metricName=\"r2\")\n",
    "print(\"R Squared (R2) on test data = %g\" % evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0322f511",
   "metadata": {},
   "source": [
    "Feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc799f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(3, {0: 0.6246, 1: 0.3355, 2: 0.0399})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.stages[-1].featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f00d739e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DecisionTreeRegressionModel: uid=dtr_977a4d87fbeb, depth=5, numNodes=63, numFeatures=3,\n",
       " DecisionTreeRegressionModel: uid=dtr_51bd1b2ea591, depth=5, numNodes=47, numFeatures=3,\n",
       " DecisionTreeRegressionModel: uid=dtr_0c0ac4993626, depth=5, numNodes=47, numFeatures=3,\n",
       " DecisionTreeRegressionModel: uid=dtr_6f7f28305192, depth=5, numNodes=45, numFeatures=3,\n",
       " DecisionTreeRegressionModel: uid=dtr_873ef539feb1, depth=5, numNodes=41, numFeatures=3,\n",
       " DecisionTreeRegressionModel: uid=dtr_80918ff0f98d, depth=5, numNodes=45, numFeatures=3,\n",
       " DecisionTreeRegressionModel: uid=dtr_29496130d791, depth=5, numNodes=45, numFeatures=3,\n",
       " DecisionTreeRegressionModel: uid=dtr_69616c038dec, depth=5, numNodes=47, numFeatures=3,\n",
       " DecisionTreeRegressionModel: uid=dtr_d209c959b8d4, depth=5, numNodes=45, numFeatures=3,\n",
       " DecisionTreeRegressionModel: uid=dtr_df17ab1fb35e, depth=5, numNodes=55, numFeatures=3,\n",
       " DecisionTreeRegressionModel: uid=dtr_5d3bed0f6948, depth=5, numNodes=43, numFeatures=3,\n",
       " DecisionTreeRegressionModel: uid=dtr_ef61c2f4f449, depth=5, numNodes=51, numFeatures=3,\n",
       " DecisionTreeRegressionModel: uid=dtr_34d72d6ef087, depth=5, numNodes=51, numFeatures=3,\n",
       " DecisionTreeRegressionModel: uid=dtr_ab87e91d97df, depth=5, numNodes=31, numFeatures=3,\n",
       " DecisionTreeRegressionModel: uid=dtr_9e480d6f3958, depth=5, numNodes=37, numFeatures=3,\n",
       " DecisionTreeRegressionModel: uid=dtr_fe3c7c0c9dad, depth=5, numNodes=43, numFeatures=3,\n",
       " DecisionTreeRegressionModel: uid=dtr_59da08cbe2cd, depth=5, numNodes=51, numFeatures=3,\n",
       " DecisionTreeRegressionModel: uid=dtr_ada4d8851a77, depth=5, numNodes=49, numFeatures=3,\n",
       " DecisionTreeRegressionModel: uid=dtr_5fa2d0853104, depth=5, numNodes=47, numFeatures=3,\n",
       " DecisionTreeRegressionModel: uid=dtr_196025c49a5f, depth=5, numNodes=35, numFeatures=3]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.stages[-1].trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d151c87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
