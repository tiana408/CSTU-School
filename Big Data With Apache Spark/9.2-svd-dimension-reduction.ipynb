{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b254713",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8d85ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-image in /home/hadoop/.local/lib/python3.9/site-packages (0.22.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /home/hadoop/.local/lib/python3.9/site-packages (from scikit-image) (2024.2.12)\n",
      "Requirement already satisfied: imageio>=2.27 in /home/hadoop/.local/lib/python3.9/site-packages (from scikit-image) (2.34.0)\n",
      "Requirement already satisfied: lazy_loader>=0.3 in /home/hadoop/.local/lib/python3.9/site-packages (from scikit-image) (0.3)\n",
      "Requirement already satisfied: networkx>=2.8 in /home/hadoop/.local/lib/python3.9/site-packages (from scikit-image) (3.2.1)\n",
      "Requirement already satisfied: numpy>=1.22 in /home/hadoop/.local/lib/python3.9/site-packages (from scikit-image) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.8 in /home/hadoop/.local/lib/python3.9/site-packages (from scikit-image) (1.12.0)\n",
      "Requirement already satisfied: packaging>=21 in /home/hadoop/.local/lib/python3.9/site-packages (from scikit-image) (23.2)\n",
      "Requirement already satisfied: pillow>=9.0.1 in /home/hadoop/.local/lib/python3.9/site-packages (from scikit-image) (10.2.0)\n",
      "\u001b[33mWARNING: Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "distutils: /home/hadoop/.local/lib/python3.9/site-packages\n",
      "sysconfig: /home/hadoop/.local/lib64/python3.9/site-packages\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = True\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/05 03:27:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/03/05 03:27:43 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-image\n",
    "\n",
    "import skimage\n",
    "\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.ml.feature import PCA, VectorAssembler\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark PCA example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5874c6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df =  spark.read.format('csv').\\\n",
    "                       options(header='true', \\\n",
    "                       inferschema='true').\\\n",
    "            load(\"data/wine_dataset.csv\",header=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdc3ff0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+-----------+--------------+---------+-------------------+--------------------+-------+----+---------+-------+-------+-----+\n",
      "|fixed_acidity|volatile_acidity|citric_acid|residual_sugar|chlorides|free_sulfur_dioxide|total_sulfur_dioxide|density|  pH|sulphates|alcohol|quality|style|\n",
      "+-------------+----------------+-----------+--------------+---------+-------------------+--------------------+-------+----+---------+-------+-------+-----+\n",
      "|          7.4|             0.7|        0.0|           1.9|    0.076|               11.0|                34.0| 0.9978|3.51|     0.56|    9.4|      5|  red|\n",
      "|          7.8|            0.88|        0.0|           2.6|    0.098|               25.0|                67.0| 0.9968| 3.2|     0.68|    9.8|      5|  red|\n",
      "|          7.8|            0.76|       0.04|           2.3|    0.092|               15.0|                54.0|  0.997|3.26|     0.65|    9.8|      5|  red|\n",
      "|         11.2|            0.28|       0.56|           1.9|    0.075|               17.0|                60.0|  0.998|3.16|     0.58|    9.8|      6|  red|\n",
      "|          7.4|             0.7|        0.0|           1.9|    0.076|               11.0|                34.0| 0.9978|3.51|     0.56|    9.4|      5|  red|\n",
      "+-------------+----------------+-----------+--------------+---------+-------------------+--------------------+-------+----+---------+-------+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- fixed_acidity: double (nullable = true)\n",
      " |-- volatile_acidity: double (nullable = true)\n",
      " |-- citric_acid: double (nullable = true)\n",
      " |-- residual_sugar: double (nullable = true)\n",
      " |-- chlorides: double (nullable = true)\n",
      " |-- free_sulfur_dioxide: double (nullable = true)\n",
      " |-- total_sulfur_dioxide: double (nullable = true)\n",
      " |-- density: double (nullable = true)\n",
      " |-- pH: double (nullable = true)\n",
      " |-- sulphates: double (nullable = true)\n",
      " |-- alcohol: double (nullable = true)\n",
      " |-- quality: integer (nullable = true)\n",
      " |-- style: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6a50d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelCol='style'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cc967ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "catcols=[]\n",
    "for i in df.dtypes:\n",
    "    if i[1] == 'string' and i[0] != labelCol:\n",
    "        catcols.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d32aea8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols=[]\n",
    "for i in df.dtypes:\n",
    "    if i[1] != 'string' and i[0] != labelCol:\n",
    "        num_cols.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2b6b9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# categorical columns\n",
    "categorical_columns = catcols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc89e8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexers = [ StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c)) for c in categorical_columns ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d07162e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders = [ OneHotEncoder(inputCol=indexer.getOutputCol(), \\\n",
    "                           outputCol=\"{0}_encoded\".format(indexer.getOutputCol())) for indexer in indexers ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "254906a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=[encoder.getOutputCol() \\\n",
    "                                       for encoder in encoders] + num_cols, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5a3f105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------+-----+\n",
      "|features                                                    |label|\n",
      "+------------------------------------------------------------+-----+\n",
      "|[7.4,0.7,0.0,1.9,0.076,11.0,34.0,0.9978,3.51,0.56,9.4,5.0]  |red  |\n",
      "|[7.8,0.88,0.0,2.6,0.098,25.0,67.0,0.9968,3.2,0.68,9.8,5.0]  |red  |\n",
      "|[7.8,0.76,0.04,2.3,0.092,15.0,54.0,0.997,3.26,0.65,9.8,5.0] |red  |\n",
      "|[11.2,0.28,0.56,1.9,0.075,17.0,60.0,0.998,3.16,0.58,9.8,6.0]|red  |\n",
      "|[7.4,0.7,0.0,1.9,0.076,11.0,34.0,0.9978,3.51,0.56,9.4,5.0]  |red  |\n",
      "+------------------------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(stages=indexers + encoders + [assembler])\n",
    "model=pipeline.fit(df)\n",
    "data = model.transform(df)\n",
    "data = data.withColumn('label',col(labelCol))\n",
    "data=data.select('features','label')\n",
    "data.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e5548cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findTopPrincipalComponentNumberSVD(data:pyspark.sql.dataframe.DataFrame=df, \\\n",
    "                                    featureCol:str='features',threshold:int=0.80) -> (int, float):\n",
    "    df_detect=data.select(featureCol)\n",
    "    df_detect_list=[]\n",
    "    for i in df_detect.collect():\n",
    "        df_detect_list.append(i[0].tolist())\n",
    "    df_detect_np=np.array(df_detect_list)\n",
    "    dfDotDfTranspose=np.matmul(df_detect_np, df_detect_np.transpose())\n",
    "    eigen_vals, eigen_vecs = np.linalg.eig(dfDotDfTranspose) #Get list of Eigenvalues and list of Eigenvectors, \n",
    "                                                             #numbef of Eigenvalues = number of the feature columns \n",
    "    tot = sum(eigen_vals)\n",
    "    var_exp = [(i / tot) for i in sorted(eigen_vals, reverse=True)] #Sort the Eigenvalues in descending order\n",
    "                                                                    #get the percentage of each Eigenvalue against \n",
    "                                                                    #sum of total eigenvalues\n",
    "                                                                    #Then find number cumulative percentage from\n",
    "                                                                    #first few largest eigenvalues to see threshold \n",
    "                                                                    #is crossed.  If so, you only need these few \n",
    "                                                                    #number of columns.\n",
    "    cumulativePercept=0\n",
    "    cumulativePerceptList=[]\n",
    "    for i in var_exp:\n",
    "        cumulativePercept+=i\n",
    "        cumulativePerceptList.append(cumulativePercept)\n",
    "        if cumulativePercept >= threshold:\n",
    "            break\n",
    "    return (len(cumulativePerceptList), float(cumulativePerceptList[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cb02a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "bestK, coveredPercentage = findTopPrincipalComponentNumberSVD(data=data, featureCol='features', threshold=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c7e2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76a4c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "coveredPercentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa16cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "# $example on$\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f50f527",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018dcd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows=data.select(\"features\").rdd.map(lambda r: [Vectors.dense(r[:])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ae3499",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = RowMatrix(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2388ee96",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = mat.computeSVD(bestK, computeU=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a2c0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "U = svd.U       # The U factor is a RowMatrix.\n",
    "s = svd.s       # The singular values are stored in a local dense vector.\n",
    "V = svd.V       # The V factor is a local dense matrix.\n",
    "# $example off$\n",
    "collected = U.rows.collect()\n",
    "print(\"U factor is:\")\n",
    "for vector in collected:\n",
    "    print(vector)\n",
    "    print(\"Singular values are: %s\" % s)\n",
    "    print(\"V factor is:\\n%s\" % V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484d8c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "uList=[]\n",
    "for i in U.rows.collect():\n",
    "    uList.append(i[:])\n",
    "uListNP=np.array(uList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4beb7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "uListNP.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a164a976",
   "metadata": {},
   "outputs": [],
   "source": [
    "S=np.diag(s.toArray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84ecf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "S.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17abc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "US=np.matmul(uListNP,S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46db65e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "US.shape #This is reduced dimension dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afbfc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "V.toArray().transpose().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df32a73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#You do not need to multiply with V transpose\n",
    "#Unless you want to project it back to original nxm space\n",
    "# USV=np.matmul(US, V.toArray().transpose()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf8ad17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#USV.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffc8cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_svd=spark.createDataFrame(pd.DataFrame(US))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990179b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_svd.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d58c913",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_svd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6283e00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=df_svd.columns, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54fea0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_svd=assembler.transform(df_svd).select(\"features\")\n",
    "df_svd.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4a3591",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label=data.select(\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699bbd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469cf7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "svdList=[]\n",
    "for i in df_svd.collect():\n",
    "    svdList.append(i[0])\n",
    "labelList=[]\n",
    "for i in df_label.collect():\n",
    "    labelList.append(i[0])\n",
    "combo=[(i[0],i[1]) for i in zip(svdList, labelList)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a728b8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.createDataFrame(combo, [\"features\",\"label\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2e8635",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c6e90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06ca93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.color import rgb2gray\n",
    "from skimage import data\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de5a181",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = data.chelsea()\n",
    "plt.imshow(cat)\n",
    "# convert to grayscale\n",
    "gray_cat = rgb2gray(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b0350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(gray_cat, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75eb1595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_cat=spark.createDataFrame(pd.DataFrame(gray_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917c2afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "# $example on$\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cefebee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed4802a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows=df_cat.rdd.map(lambda r: [Vectors.dense(r[:])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feabed8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = RowMatrix(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bd620f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9219b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=df_cat.columns\n",
    "assembler = VectorAssembler(inputCols=cols, outputCol=\"features\")\n",
    "data=assembler.transform(df_cat)\n",
    "bestK, coveredPercentage = findTopPrincipalComponentNumberSVD(data=data, featureCol='features', threshold=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ed71c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1d10cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = mat.computeSVD(bestK, computeU=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bcaff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "U = svd.U       # The U factor is a RowMatrix.\n",
    "s = svd.s       # The singular values are stored in a local dense vector.\n",
    "V = svd.V       # The V factor is a local dense matrix.\n",
    "# $example off$\n",
    "collected = U.rows.collect()\n",
    "print(\"U factor is:\")\n",
    "for vector in collected:\n",
    "    print(vector)\n",
    "    print(\"Singular values are: %s\" % s)\n",
    "    print(\"V factor is:\\n%s\" % V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873e0a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "uList=[]\n",
    "for i in U.rows.collect():\n",
    "    uList.append(i[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0438e2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "uListNP=np.array(uList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa32d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "uListNP[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfeddc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8a15e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "S=np.diag(s.array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9183144c",
   "metadata": {},
   "outputs": [],
   "source": [
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe973b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "US=np.matmul(uListNP,S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60daee5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "US.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646ab721",
   "metadata": {},
   "outputs": [],
   "source": [
    "USV=np.matmul(US, V.toArray().transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb6e633",
   "metadata": {},
   "outputs": [],
   "source": [
    "USV.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5501784a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(USV, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bf3662",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(gray_cat, cmap=\"gray\") #Original picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd7fc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressedSize=uListNP.size+s.size+V.toArray().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216b1f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "originalSize=gray_cat.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cded104e",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressionRate=compressedSize/originalSize*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f234aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressionRate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9740595f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank=min(USV.shape[0], bestK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a101fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f434e653",
   "metadata": {},
   "outputs": [],
   "source": [
    "coveredPercentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4308a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
