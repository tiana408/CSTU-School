{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff90cfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300c5e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is one time on your PC; you may need to run it everytime on colab and databrick notebooks\n",
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06a4ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167306f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(conf=SparkConf())\n",
    "spark = SparkSession(sparkContext=sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839a0369",
   "metadata": {},
   "source": [
    "### Linear Support Vector Machine with pyspark¶\n",
    "\n",
    "Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84d3ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('data/SparkData/bank.csv', header=True, inferSchema=True, sep=\";\")\n",
    "df.drop('day','month','poutcome').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075b10f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669cf677",
   "metadata": {},
   "source": [
    "### Deal with categorical data and Convert the data to dense vector¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cd7b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "catcols = ['job','marital','education','default','housing','loan','contact','poutcome']\n",
    "num_cols = ['balance', 'duration','campaign','pdays','previous']\n",
    "labelCol = 'y'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbe9796",
   "metadata": {},
   "source": [
    "### Process categorical columns\n",
    "\n",
    "The following code does three things with pipeline:\n",
    "\n",
    "StringIndexer all categorical columns\n",
    "\n",
    "OneHotEncoder all categorical index columns\n",
    "\n",
    "VectorAssembler all feature columns into one vector column\n",
    "\n",
    "Categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e2442c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# categorical columns\n",
    "categorical_columns = catcols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c00654f",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexers = [ StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c)) for c in categorical_columns ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d3761d",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders = [ OneHotEncoder(inputCol=indexer.getOutputCol(), \\\n",
    "                           outputCol=\"{0}_encoded\".format(indexer.getOutputCol())) for indexer in indexers ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a598190",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=[encoder.getOutputCol() \\\n",
    "                                       for encoder in encoders] + num_cols, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3391d049",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=indexers + encoders + [assembler])\n",
    "model=pipeline.fit(df)\n",
    "data = model.transform(df)\n",
    "data = data.withColumn('label',col(labelCol))\n",
    "data=data.select('features','label')\n",
    "data.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2f5d9f",
   "metadata": {},
   "source": [
    "### We need to deal with label, which is string, yes or no, need to make them numbers\n",
    "\n",
    "Build StringIndexer stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20bdf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index labels, adding metadata to the label column \n",
    "labelIndexer = StringIndexer(inputCol='label',\n",
    "                             outputCol='indexedLabel').fit(data)\n",
    "data=labelIndexer.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbefde62",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277e9c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorIndexer\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous. \n",
    "featureIndexer =VectorIndexer(inputCol=\"features\", \\\n",
    "                                  outputCol=\"indexedFeatures\", \\\n",
    "                                  maxCategories=3).fit(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1e6e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=featureIndexer.transform(data)\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49107c78",
   "metadata": {},
   "source": [
    "### Split the data to training and test data sets¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2b48f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets (40% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.6, 0.4])\n",
    "trainingData.show(5,False)\n",
    "testData.show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd9692a",
   "metadata": {},
   "source": [
    "### Build cross-validation model¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b2751a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LinearSVC\n",
    "lsvc = LinearSVC(featuresCol=\"indexedFeatures\", labelCol=\"indexedLabel\", maxIter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dc72cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert indexed labels back to original labels.\n",
    "from pyspark.ml.feature import IndexToString\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                               labels=labelIndexer.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b5d89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain indexers and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[lsvc,labelConverter])\n",
    "# Train model.  This also runs the indexers.\n",
    "lsvcModel = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379edfee",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2dc29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions.\n",
    "predictions = lsvcModel.transform(testData)\n",
    "# Select example rows to display. \n",
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51f0eb5",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522378cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy = {accuracy}\")\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc39cb83",
   "metadata": {},
   "source": [
    "### Get confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11414571",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=predictions.select(\"prediction\").collect()\n",
    "y_orig=predictions.select(\"indexedLabel\").collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4632ece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_orig, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e857ba",
   "metadata": {},
   "source": [
    "### Here is the slope of the hyper-plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1f5639",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsvcModel.stages[0].coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11224a85",
   "metadata": {},
   "source": [
    "### Here is intercept of the hyper-plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4af871",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsvcModel.stages[0].intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf898b6",
   "metadata": {},
   "source": [
    "### Tear down machine learning pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e31d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop session \n",
    "sc.stop()  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
